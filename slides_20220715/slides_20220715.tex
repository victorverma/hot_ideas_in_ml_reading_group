\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{AnnArbor}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{hyperref}
\hypersetup{colorlinks=true}
\usepackage{graphicx}
\graphicspath{{./figures/}}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}

\title[Bayesian Neural Networks]{
Bayesian Neural Networks
}
%\author{Victor Verma}
\author[Victor Verma]{Victor Verma}
\institute[]{Hot Ideas in Machine Learning Reading Group, University of Michigan}
\date{7/15/22}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

%\section{Introduction}

\begin{frame}{Introduction}
    For a deep neural network, there can be many parameter settings for which the fit to the training data is good. Different settings can give rise to very different fitted models. It turns out that we can achieve good generalization by combining these models instead of using just one. \textbf{Bayesian neural networks (BNNs)} provide a means for doing this.
    
    \medskip
    
    We start with a prior $p(\boldsymbol{\theta})$ over the parameters. We then compute the posterior $p(\boldsymbol{\theta} | \mathcal{D})$ and use Bayesian model averaging to compute the posterior predictive density
    \begin{equation*}
        p(\boldsymbol{y} | \boldsymbol{x}, \mathcal{D}) = \int p(\boldsymbol{y} | \boldsymbol{x}, \boldsymbol{\theta})p(\boldsymbol{\theta} | \mathcal{D})\,d\boldsymbol{\theta}.
    \end{equation*}
\end{frame}

\begin{frame}{Gaussian priors}
    Let
    \[
    f(\boldsymbol{x}; \boldsymbol{\theta}) = \boldsymbol{W_L}(\cdots\phi(\boldsymbol{W_1}\boldsymbol{x} + \boldsymbol{b_1})) + \boldsymbol{b_L};
    \]
    this is a multi-layer perceptron (MLP) with $L - 1$ layers and activation function $\phi$. When $L = 1$, this becomes
    \[
    f(\boldsymbol{x}; \boldsymbol{\theta}) = \boldsymbol{W_2}\phi(\boldsymbol{W_1}\boldsymbol{x} + \boldsymbol{b_1}) + \boldsymbol{b_2}.
    \]
    Specifying a prior entails specifying the distributions of the $\boldsymbol{W}_{\ell}$'s and the $\boldsymbol{b}_{\ell}$'s. Typically, Gaussian priors are used: $\boldsymbol{W}_{\ell} \sim \mathcal{N}(\boldsymbol{0}, \alpha_{\ell}^2\boldsymbol{I})$ and $\boldsymbol{b}_{\ell} \sim \mathcal{N}(\boldsymbol{0}, \beta_{\ell}^2\boldsymbol{I})$ for $\ell = 1, \ldots, L$.
\end{frame}

\begin{frame}{Gaussian priors}
    Let $L = 2$, let $\phi$ be the sigmoid function, and let the input and output be linear. Each panel of Fig.~\ref{fig:mlp_priors} shows a sample of MLPs from a particular Gaussian prior.
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{mlp_priors}
        \caption{Samples of MLPs from two different Gaussian priors}
        \label{fig:mlp_priors}
    \end{figure}
\end{frame}

\begin{frame}{Learning the prior}
    In more general situations, there are a few approaches that can be taken to choosing a prior:
    \begin{itemize}
        \item Cross-validation can be used to identify a good choice of hyperparameters, but it can be slow if there are many hyperparameters.
        \item An empirical Bayes approach can be taken. E.g., if the hyperparameters are $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$, we could find the values that maximize $\log p(\mathcal{D} | \boldsymbol{\alpha}, \boldsymbol{\beta}) = \int \log p(\mathcal{D} | \boldsymbol{\theta})p(\boldsymbol{\theta} | \boldsymbol{\alpha}, \boldsymbol{\beta})\,d\boldsymbol{\theta}$. However, this can be computationally difficult.
        \item A third approach is Bayesian transfer learning: estimating the posterior using one dataset and then using it as the prior with another dataset.
    \end{itemize}
\end{frame}

\begin{frame}{Likelihoods for BNNs}
    There are two ways to transform the likelihood $p(\boldsymbol{y} | \boldsymbol{x}, \boldsymbol{\theta})$ to get better predictive accuracy. Ordinarily we would have $p(\boldsymbol{\theta} | \mathcal{D}) \propto p(\boldsymbol{y} | \boldsymbol{x}, \boldsymbol{\theta})p(\boldsymbol{\theta})$, or $\log p(\boldsymbol{\theta} | \mathcal{D}) = \log p(\boldsymbol{y} | \boldsymbol{x}, \boldsymbol{\theta}) + \log p(\boldsymbol{\theta}) + \text{const}$.
    \begin{itemize}
        \item One approach is to use a \textbf{tempered posterior}: $\log p_{\text{tempered}}(\boldsymbol{\theta} | \mathcal{D}) = \alpha\log p(\boldsymbol{y} | \boldsymbol{x}, \boldsymbol{\theta}) + \log p(\boldsymbol{\theta}) + \text{const}$.
        \item Another approach is to use a \textbf{cold posterior}: $\log p_{\text{cold}}(\boldsymbol{\theta} | \mathcal{D}) = \frac{1}{T}\log p(\boldsymbol{y} | \boldsymbol{x}, \boldsymbol{\theta}) + \frac{1}{T}\log p(\boldsymbol{\theta}) + \text{const}$.
    \end{itemize}
    Model selection methods like cross-validation can be used to choose $\alpha$ and $T$.
\end{frame}

\begin{frame}{Posteriors for BNNs}
    There are several ways to approximate the posterior:
    \begin{itemize}
        \item Laplace approximation
        \item Variational inference
        \item Expectation propagation
        \item Last layer methods
        \item Dropout
        \item MCMC methods
        \item Methods based on the SGD trajectory
        \item Deep ensembles
    \end{itemize}
\end{frame}

\begin{frame}{Laplace approximation}
    Write the neural network as a function $\boldsymbol{f}(\boldsymbol{x}; \boldsymbol{\theta}) \in \mathbb{R}^C$, where $\boldsymbol{\theta} \in \mathbb{R}^P$. Let $\boldsymbol{J}_{\boldsymbol{\theta}}(\boldsymbol{x}) \in \mathbb{R}^{C \times P}$ be the Jacobian of $\boldsymbol{f}$. Let $\boldsymbol{\Lambda}(\boldsymbol{y}; \boldsymbol{f}) = -\nabla_{\boldsymbol{f}}^2\log p(\boldsymbol{y} | \boldsymbol{f})$; this is a per-input noise term.
    
    \medskip

    Suppose that we use a Gaussian prior, $p(\boldsymbol{\theta}) = \mathcal{N}(\boldsymbol{\theta} | \boldsymbol{m}_0, \boldsymbol{S}_0)$. The Laplace approximation of the posterior $p(\boldsymbol{\theta} | \mathcal{D})$ is the $\mathcal{N}(\boldsymbol{\theta}^*, \boldsymbol{\Sigma}_{\text{GGN}})$ distribution, where $\boldsymbol{\theta}^*$ is the MAP estimate and
    \[
    \boldsymbol{\Sigma}_{\text{GGN}} = \left(\sum_{n = 1}^N \boldsymbol{J}_{\boldsymbol{\theta}^*}(\boldsymbol{x}_n)^T\boldsymbol{\Lambda}(\boldsymbol{y}_n; \boldsymbol{f}_n)\boldsymbol{J}_{\boldsymbol{\theta}^*}(\boldsymbol{x}_n) + \boldsymbol{S}_0^{-1}\right)^{-1}.
    \]
\end{frame}

\begin{frame}{Laplace approximation}
    Advantages of Laplace approximation:
    \begin{itemize}
        \item It's simple.
        \item It can be used on a pretrained model.
    \end{itemize}
    Disadvantages:
    \begin{itemize}
        \item The approximation produced is highly local, because it's based on the Hessian at the MAP. This means that it can suffer badly from local optima - the approximation can be good locally, but bad overall.
    \end{itemize}
\end{frame}

\begin{frame}{Methods based on the SGD trajectory}
    Under certain assumptions, with a fixed learning rate, iterates produced by stochastic gradient descent can be viewed as samples from a Gaussian approximation $\mathcal{N}(\boldsymbol{\theta} | \hat{\boldsymbol{\theta}}, \boldsymbol{\Sigma})$ to $p(\boldsymbol{\theta} | \mathcal{D})$, where $\hat{\boldsymbol{\theta}}$ is a local mode. With a constant learning rate, the sequence of iterates $\{\boldsymbol{\theta}_s\}$ doesn't converge to a local optimum. However, they do lie along the periphery of a point of good generalization. \textbf{Stochastic Weight Averaging (SWA)} computes $\bar{\boldsymbol{\theta}} = \sum_{s = 1}^S \boldsymbol{\theta}_s$, which should produce better generalization.
    
    \begin{figure}
        \centering
        \includegraphics[scale=0.5]{stochastic_weight_averaging}
        \caption{An example instance of SWA}
        \label{fig:stochastic_weight_averaging}
    \end{figure}
\end{frame}

\begin{frame}{Methods based on the SGD trajectory}
    \textbf{Stochastic Weight Averaging with Gaussian Posterior (SWAG)} is an extension of SWA that approximates the posterior using a Gaussian distribution centered at the SWA solution. The approximation is $\mathcal{N}(\boldsymbol{\theta} | \hat{\boldsymbol{\theta}}, \boldsymbol{\Sigma})$, where $\hat{\boldsymbol{\theta}}$ is as on the previous slide and $\boldsymbol{\Sigma}$ is the sum of a diagonal matrix and a low-rank matrix computed from $\boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_S$. SWAG can be used on large networks and offers improved accuracy.
\end{frame}

\begin{frame}{Deep ensembles}
    Some methods approximate $p(\boldsymbol{\theta} | \mathcal{D})$ in a neighborhood of one of its modes. However, the posterior for a deep neural network could have many modes. Fitted functions at the same mode may be quite similar, while fitted functions at different modes may be very different. So, focusing on one mode can result in underestimation of uncertainty and poor generalization.
    
    \medskip
    
    The method of \textbf{deep ensembles} instead uses multiple modes $\hat{\boldsymbol{\theta}}_1, \ldots, \hat{\boldsymbol{\theta}}_M$:
    \[
    p(\boldsymbol{\theta} | \mathcal{D}) \approx \frac{1}{M}\sum_{m = 1}^M \delta(\boldsymbol{\theta} - \hat{\boldsymbol{\theta}}_m).
    \]
    The multiple modes can be obtained by varying random seeds, hyperparameters, or architecture.
\end{frame}

\begin{frame}{Deep ensembles}
    Below are some extensions of deep ensembles:
    \begin{itemize}
        \item \textbf{Multi-SWAG} uses Gaussian distributions instead of point masses:
        \[
        p(\boldsymbol{\theta} | \mathcal{D}) \approx \frac{1}{M}\sum_{m = 1}^M \mathcal{N}(\boldsymbol{\theta} | \hat{\boldsymbol{\theta}}_m, \boldsymbol{\Sigma}_m).
        \]
        This makes it possible to generate arbitrarily many posterior samples.
        \item \textbf{Bootstrap sampling} trains different ensemble members on different subsets of the data to increase diversity in the predictions.
    \end{itemize}
\end{frame}

\begin{frame}{Generalization in Bayesian deep learning}
    "Being Bayesian" can improve
    \begin{itemize}
        \item Predictive accuracy
        \item Generalization performance
    \end{itemize}
\end{frame}

\begin{frame}{Sharp vs flat minima}
    A minimum of a loss function can be \textbf{sharp} or \textbf{flat}.
    \begin{itemize}
        \item At a sharp minimum, the function has a narrow, deep hole (Fig.~\ref{fig:sharp_minimum}).
        \item At a flat minimum, it has a broad, shallow hole (Fig.~\ref{fig:flat_minimum}).
    \end{itemize}
    
    \begin{figure}
        \centering
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{sharp_minimum}
            \caption{A sharp minimum}
            \label{fig:sharp_minimum}
        \end{subfigure}
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{flat_minimum}
            \caption{A flat minimum}
            \label{fig:flat_minimum}
        \end{subfigure}
        \caption{Sharp and flat minima}
        \label{fig:sharp_and_flat_minima}
    \end{figure}
\end{frame}

\begin{frame}{Sharp vs flat minima}
    Flat minima are preferable to sharp minima for two reasons:
    \begin{itemize}
        \item A sharp minimum corresponds to a tiny loss, which is typically caused by overfitting.
        \item Flat minima are more robust and generalize better. This can seen in two ways:
            \begin{itemize}
                \item A flat minimum corresponds to a region in parameter space with high posterior uncertainty; samples from the region don't memorize irrelevant details of the training set.
                \item A flat minimum has a short description length, i.e., few bits are needed to specify its location. This results in better generalization.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Sharp vs flat minima}
    SGD (Stochastic Gradient Descent) tends to avoid sharp minima:
    \begin{itemize}
        \item A sharp minimum is in a region $\mathcal{A}$ of low evidence.
        \item The evidence for $\mathcal{A}$ is proportional to $\int_{\mathcal{A}} e^{-\mathcal{L}(\boldsymbol{\theta})}\,d\boldsymbol{\theta}$, so this integral is small.
        \item The integral is proportional to SGD's probability of entering $\mathcal{A}$, so SGD is unlikely to enter $\mathcal{A}$.
    \end{itemize}
    This property is due to SGD's use of noise.
\end{frame}

\begin{frame}{Effective dimensionality of a model}
    The \textbf{effective dimensionality} of a model reflects how many well-determined parameters it has. It is defined as
    \begin{equation*}
        N_{\text{eff}}(\boldsymbol{H}, c) = \sum_{i = 1}^k \frac{\lambda_i}{\lambda_i + c},
    \end{equation*}
    where $\boldsymbol{H}$ is the Hessian of the loss at the appropriate local mode, $\lambda_1, \ldots, \lambda_k$ are $\boldsymbol{H}$'s eigenvalues, and $c$ is a regularization constant.
    
    \medskip
    
    A low effective dimensionality implies that significant compression is possible. Also, the effective dimensionality is a good proxy for generalization.
\end{frame}

\begin{frame}{Effective dimensionality of a model}
    Fig.~\ref{fig:effective_dimensionality_and_test_loss} illustrates the last point on the previous slide. For CNNs with near-zero training loss, the pattern in the test loss closely resembles that in the effective dimensionality.
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{effective_dimensionality_and_test_loss}
        \caption{Effective dimensionality and test loss for various CNNs trained on the CIFAR-100 image dataset. CNNs above the green curve have near-zero training loss. CNNs on the same yellow curve have the same number of parameters.}
        \label{fig:effective_dimensionality_and_test_loss}
    \end{figure}    
\end{frame}

\end{document}